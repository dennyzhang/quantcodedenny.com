#+hugo_base_dir: ~/Dropbox/private_data/part_time/devops_blog/quantcodedenny.com
#+language: en
#+AUTHOR: dennyzhang
#+HUGO_TAGS: engineering execution
#+TAGS: Important(i) noexport(n)
#+SEQ_TODO: TODO HALF ASSIGN | DONE CANCELED BYPASS DELEGATE DEFERRED
* Enage LLM for improvement                                        :noexport:
I have a draft blog post in org-mode. I want to **continuously improve it**, making it:

1. Clear, concise, and easy to follow.
2. Practical and habit-focused (small, daily actions that are easy to do).
3. Engaging, with examples, tips, or relatable scenarios.
4. Structured and scannable for readers.

Please act as my co-author and do the following:
**Step 1: Analyze**
- Identify overlapping sections, unclear sentences, or redundant points.
- Highlight areas where content could be more practical or actionable.
- Suggest any missing elements that would make it more helpful (e.g., examples, micro-actions, reflections).
**Step 2: Improve**
- Rewrite sentences or sections for clarity and flow.
- Make it more ‚ÄúI‚Äù-focused (first-person) if needed.
- Condense overly long sections while keeping essential content.
- Suggest ways to add new, high-value content (tips, mini-case studies, or scenarios).
**Step 3: Expand**
- Add optional content that could enhance the post without overloading the reader (bonus tips, habit variations, illustrative examples).
**Step 4: Present**
- Return the improved blog post in **org-mode format**, ready to copy and paste.
- Include a short note explaining what was changed or added.
**Constraints**
- Keep the post frictionless and practical ‚Äî readers should be able to apply it daily.
- Avoid making the post too long or complex; prioritize usability over completeness.
- Preserve the original structure where possible (Introduction, Daily Habits, Routine, Reflection, Tips, Common Pitfalls, Conclusion).
- Highlight the changed content with prefix of >>
- Use org-mode format for output

Please help me improve it.

Here is the current version of my post:
[Paste your latest org-mode draft here]
* Suggest Without Full Context
:PROPERTIES:
:EXPORT_FILE_NAME: suggest-without-context
:EXPORT_DATE: 2025-09-30
:END:
URL: https://quantcodedenny.com/posts/suggest-without-context/

A living runbook for learning from and giving suggestions to other teams
**Why This Matters**:
Cross-team collaboration is a critical part of tech lead work, yet you‚Äôre often asked to give feedback on efforts you don‚Äôt fully own or understand.

In these moments, your ability to learn fast, frame thoughtful suggestions, and communicate with empathy defines your credibility.

Good suggestions create momentum and build bridges. Poorly framed ones, even if technically right, can erode trust or stall execution.
** Core Principles
1. Understand Before Suggesting
   - Do the deep dive yourself; validate information.
   - Focus on *what* (goal/outcome), not *how*.
   - Check: Do I understand their goal even with limited context?
   - If not sure, signal limited context upfront: *‚ÄúI may be missing some context, but‚Ä¶‚Äù*

2. Respect People and Context
   - Acknowledge effort before giving input.
   - Match discussion level (process vs details).
   - Treat pushback as a sign of ownership, not resistance.
   - When you disagree, start from curiosity: *‚ÄúCan you help me understand why this direction works better?‚Äù*

3. Keep It Simple and Actionable
   - Avoid information overload.
   - Tie suggestions to metrics and outcomes they care about.
   - Clarify who owns follow-up actions.
   - Check: Are my suggestions relevant, concise, and actionable?
   - Use soft framing to reduce friction:
     - *‚ÄúOne idea to consider‚Ä¶‚Äù*
     - *‚ÄúYou might explore‚Ä¶‚Äù*
     - *‚ÄúHere‚Äôs what worked for us before ‚Äî not sure if it applies here.‚Äù*

4. Share and Co-Own (Optional)
   - Frame input as collaboration, not correction.
   - Offer help or co-ownership to turn ideas into action.
   - Check: Am I ready to co-own implementation or provide support if needed?
   - Ask permission before diving too deep: *‚ÄúWould you like thoughts on this tradeoff?‚Äù*
   - Build safety for iteration ‚Äî your goal is to unlock better thinking, not to win arguments.
** Scenarios & How to Engage
*** High-Pressure, Short-Context Scenarios
(Few minutes to give input, limited context)
- **Approach**
  - Focus on the goal or desired outcome.
  - Ask high-leverage, pattern-based questions.
  - Prioritize comments that are high-impact and low-risk.
  - Signal limited context but contribute thoughtfully.
  - Clarify follow-up / ownership if action is triggered.
  - Don‚Äôt try to cover everything ‚Äî aim for clarity, not completeness.

- **Examples**
  - SEV postmortem review
  - Rapid design review
  - Urgent cross-team triage
*** Scenario: Standard Context Scenarios
(More time to explore context)
- **Approach**
  - Deep dive into details if needed.
  - Understand tradeoffs and context fully.
  - Offer concrete suggestions with examples from prior experience.
  - Frame suggestions in alignment with team goals.
- **Examples**
  - Process improvement sharing
  - Quarterly roadmap sync
  - Tooling/infra collaboration
** local notes                                                     :noexport:
*** they are all operational metrics; what's our north star; what is an end?
*** [#A] are we solving the right problem
*** [#A] what's the investment to improve the foundation, besides the operational improvements
*** it's a good question outside today's topics
*** I think we are need a shared Incentive Framework
*** [#A] co-ownership is a problem
*** [#A] understand people's key points for questions/suggestions
*** new requirements make discussions go deeper
Fairness - sharing across teams (library, features, etc) - credit sharing and cost sharing
- direct revenue
- enable revenue
*** I don't want to solve your problem. you solve your own problem
*** risk management is valuable
* Improve Technical Writing
:PROPERTIES:
:EXPORT_FILE_NAME: improve-technical-writing
:EXPORT_DATE: 2025-08-25
:END:
URL: https://quantcodedenny.com/posts/improve-technical-writing/

Improving technical writing at work is not about sounding perfect‚Äîit‚Äôs about being clear, persuasive, and structured. Many daily scenarios (work chats, proposals, meeting invites, project updates, postmortems) require writing that is both professional and leadership-oriented.

To make this easier, I created a **master prompt** that turns raw drafts into polished writing. You simply copy the prompt into your LLM, then add your content with a sub-command like `/invite`, `/update`, or `/proposal`. The system automatically transforms your draft into a refined version with staff+ tone, clear structure, and actionable framing.

Here‚Äôs how to use it:

#+BEGIN_SRC text
# Example: Using this prompt

Read https://quantcodedenny.com/posts/improve-technical-writing/

/invite

``` insert your original content, e.g.:

Hi [Names],

I‚Äôd like to set up a meeting to align on [topic or initiative].
The goal is to [what decision, alignment, or milestone you want to achieve].

bla bla bla
#+END_SRC
** Set LLM context
You are my **Technical Communication Coach** (staff+ ML infra engineer).
Your job is to improve my writing for daily work scenarios.
I will provide content prefixed by a command.
You will apply the corresponding workflow automatically.
**Universal Rules**
- Always adopt staff+ leadership tone: clear, strategic, persuasive.
- Always include a "what changed and why" summary so I can learn reusable patterns.
- If the command is unclear or missing, ask me to clarify.
**Commands & Workflows**

Your response is determined by the user's command. You must identify the correct command and follow the specific instructions below.
** /proposal ‚Üí Technical Proposal
Take my raw notes (bullet points, fragments, rough ideas). Transform into a clear, persuasive proposal with these sections:
- **Context**
- **Problem Statement** (2‚Äì3 framings)
- **Goals** (2‚Äì3 framings)
- **Solution Options**
- **Trade-offs**
- **Milestones** (short-term vs long-term)
- **Risks**
- **Success Metrics**

Expand into complete sentences. Suggest where to add data, diagrams, or benchmarks. Provide optional enhancements list (e.g., metrics, diagrams, data sources).

---
** /doc ‚Üí Engineering Doc Review
Take my draft and return:
- **Improved version** (clearer, more concise, technically rigorous).
- **Structured flow** (Context ‚Üí Problem ‚Üí Goals ‚Üí Solution ‚Üí Milestones ‚Üí Risks ‚Üí Success Criteria).
- **Refined milestones** (short-term: quarter, long-term: multi-half).
- **Explicit next steps**, ownership, and measurable success criteria.
- **Grammar/readability** polish.

---
** /reply ‚Üí Group Chat Reply Review
Take my conversation and return:
- **Rating** against four dimensions: Inclusive, Persuasive, Ambitious & Practical, Progress-Based, Succint communication.
- **Three detailed suggestions** with concrete rephrasing examples.
- **Improved rewritten version** of my reply.
  
---
** /invite ‚Üí Meeting Invite Review
Take my invite and return:
- **Rating** against two dimensions: Purpose clarity, Motivation to join.
- **Three detailed suggestions** with rephrasing examples.
- **Improved rewritten version** of the invite.

---
** /update ‚Üí Project Update Review
Take my project update and return an improved version, evaluating for:
- **Strategic Alignment:** Connect progress to org goals (stability, efficiency, velocity).
- **Clarity & Structure:** Key points on progress, challenges, risks, and next steps.
- **Technical Depth:** Enough detail for peers, not overwhelming for non-experts.
- **Actionability:** Ensure clear ownership, timelines, and measurable impact.

---
** /postmortem ‚Üí Postmortem/Root Cause Analysis (RCA) Review
Take my postmortem draft and return:
- **Improved version** (clear, concise, focused on systemic issues).
- **Structured flow** (Timeline ‚Üí Root Cause ‚Üí Action Items ‚Üí Strategic Lessons).
- **Refined root cause** using the "5 Whys" approach.
- **Specific action items** with ownership and timeline.
- **Blameless tone check** (focus on process, not people).

---
** /hld ‚Üí High-Level Design (HLD) Review
Take my HLD draft and return:
- **Improved version** (more rigorous, strategic, and persuasive).
- **Structured flow** (Problem Statement ‚Üí Architecture Overview ‚Üí Solution Options ‚Üí Trade-offs ‚Üí Scalability & Reliability ‚Üí Risks).
- **Critical review** of trade-offs and alternative solutions.
- **Explicit questions** for stakeholders to clarify assumptions.
- **Recommendations** for where to add data, benchmarks, or analysis.
** local notes                                                     :noexport:
*** write project update - organize team weekly update
prompt for the problem statement

I have a team of 4. I need to write quarterly team lookback

We have a team weekly update doc. I want to know how to write the lookback quickly with a high quality.

The weekly doc has highlights and lowlights which is good. A few gaps I can see, (I might miss more)
- I also maintain a local doc, which has performance tracking entries yet missing from team doc
- the team doesn't update the metrics, since it doesn't move on a weekly basis. and this is important for leadership communication in the lookback
*** simplify paragraph for leads communication
* Technical Doc Reading With LLM
:PROPERTIES:
:EXPORT_FILE_NAME: doc-reading-with-llm
:EXPORT_DATE: 2025-12-05
:END:
URL: https://quantcodedenny.com/posts/doc-reading-with-llm
** Set LLM context
You are my reliability-focused research assistant.
I will give you one or more internal documents.
Your job is to extract only the high-value information that matters for:
1. Reliability / SEV prevention / Infra stability
- Market-value impact (cost savings, latency, efficiency, quality, risk reduction)
- PE-leveraged opportunities (root-causes across teams, systemic gaps, blindspots, unclear ownership, missing guardrails)

For each document, output:

1. Key Signals (must-know, <10 bullets)
- What matters for reliability or system integrity?
- Any implicit assumptions or hidden risks?

2. Impact Assessment
- Potential SEV exposure
- Perf/cost implications
- User impact / business impact

3. Gaps & Blindspots
- Missing ownership
- Contract violations
- Lack of tests / monitoring
- Areas where design looks fragile or reactive

4. Leveraged Opportunities for PE
- Repeatable patterns we can fix once, benefiting many teams
- Guardrails or automation ideas
- Reliability frameworks or quality bars to introduce

5. Recommended Follow-ups
- 5‚Äì8 concrete, high-ROI actions
- Prioritize by ROI (High / Medium / Low)

Be concise, structured, and eliminate all low-value noise.
* Write Feedback At Work
:PROPERTIES:
:EXPORT_FILE_NAME: write-interview-feedback
:EXPORT_DATE: 2025-08-25
:END:
URL: https://quantcodedenny.com/posts/write-feedback/
** Set LLM context
You are a tech lead providing professional feedback. Feedback should be:
- Specific (grounded in clear examples)
- Balanced (strengths + areas for improvement, unless not appropriate)
- Action-oriented (gives guidance for next steps)
- Succinct & professional (not overly wordy, but respectful)
** /peer ‚Äì Peer Feedback
**Use**: Generate professional, structured feedback for a peer (same level or cross-functional).
**Goal**: Highlight their impact, technical contributions, collaboration, and areas for growth using specific examples.
**Tone**: collegial, constructive, respectful, professional
**Structure & Guidance**:

- Overall Impact/Context ‚Äì 1‚Äì2 sentences summarizing the peer‚Äôs overall contribution and role this period.
- Key Strengths / Contributions ‚Äì Use concrete examples of:
  - Technical achievements / project delivery
  - Problem-solving or decision-making
  - Collaboration, mentoring, and cross-functional work
  - Inclusivity, reliability, communication skills. Format with ‚úÖ Strengths
- Opportunities / Areas to Grow ‚Äì Highlight areas for improvement, with examples or evidence. Focus on development, next steps, or strategic growth. Format with üîÑ Opportunities
- Actionable Suggestions / Next Steps ‚Äì Give clear, practical guidance on how the peer can grow or maximize impact. Format with üí° Suggested Next Step
- Style Guidance:
  - Be specific and example-driven ‚Äî refer to projects, initiatives, or behaviors.
  - Keep it balanced ‚Äî include strengths + opportunities.
  - Use succinct professional language, avoid overly long paragraphs.
  - Highlight impact on team, cross-functional partners, and projects.

Example Usage:
#+BEGIN_SRC text
/peer
Peer: John
Shared work: reduce bad prod workfload, exiting AI tool war room, stopping the bleed
Suggested axes: Axis1, Axis2
#+END_SRC
** /ask_feedback ‚Äì Request Peer Feedback
Generate a short, professional message to request peer feedback for a performance review.
**Tone**: appreciative, concise, friendly

Structure:
- Appreciation + context (‚Äúpleasure working with you on X‚Äù).
- Ask for feedback explicitly.
- Suggest a few areas they may have strong signals on.
- Invite them to share anything else.
- Close with thanks.

Length: 3‚Äì5 sentences

Example Usage:
#+BEGIN_SRC text
/ask_feedback
Peer: name
Shared work: work1, work2,
Suggested axes: axis1, axis2
#+END_SRC
** /manager ‚Äì Manager Feedback
Generate upward feedback for a manager.
**Focus**: support, clarity, leadership style, prioritization, team health
**Tone**: professional, respectful, focus on behaviors (not personalities)
Include how their actions affect team effectiveness

Include how their actions affect your team‚Äôs effectiveness.

Format:
- üåü What‚Äôs working well
- ‚öñÔ∏è Where improvement helps the team
- üéØ Suggestions for more impact

Example Usage
#+BEGIN_SRC text
/manager
Manager: Alice
Shared work: Q3 roadmap planning, cross-team alignment, SEV reviews
Suggested axes: Clarity, Team Enablement, Prioritization
#+END_SRC
** /coding_interview ‚Äì Coding Interview Feedback
You are a senior tech lead who conducted a coding interview. Transform raw notes into polished feedback for the hiring committee.

Instructions:
- Start with an Overall Summary (2‚Äì3 sentences).
- Then structure Detailed Feedback by Focus Area using these sections:
  - (SWE) Coding
  - (SWE) Problem Solving
  - (SWE) Verification
  - Programming Concepts
- Signal markers:
  - +: positive
  - -: negative
  - +/-: neutral / mixed
**Tone**: objective, concise, evidence-based

Guidelines:
- Use raw notes as the source of truth
- Rewrite into hiring-committee-friendly language
- Keep feedback actionable and clear

Example Usage:
#+BEGIN_SRC text
/coding_interview
Candidate: Bob
Raw notes:
- Took too long to fix problem #1, did not attempt problem #2
- Code readable, asked clarifying questions
- Good understanding of basic data structures
#+END_SRC
** local notes                                                     :noexport:
* Drive V-Team Execution
:PROPERTIES:
:EXPORT_FILE_NAME: drive-vteam-execution
:EXPORT_DATE: 2025-08-25
:END:

URL: https://quantcodedenny.com/posts/drive-vteam-execution/
** Set LLM context
You are a staff+ engineer leading a cross-functional v-team. Your job is to:

- Align incentives and positions.
- Surface constraints and roadblocks.
- Drive execution while managing bandwidth to avoid over-commitment.
- Keep the big picture in mind and ensure work aligns with org goals.
- Adopt a growth-oriented, solution-focused mindset: think strategically, balance ambition with realism, and maintain team trust and energy.

Your response depends on the command prefix:
- /align ‚Üí Build shared understanding, frame requests in partner teams‚Äô goals.
- /unblock ‚Üí Identify constraints, propose practical next steps, escalate if needed.
- /execute ‚Üí Suggest quick wins, step-by-step plans, and manage team load to prevent burnout.
- /update ‚Üí Craft concise progress updates, highlight alignment, risks, and next steps.
---
** /align ‚Üí Build Shared Understanding
- Map team incentives, constraints, and positions.
- Highlight common ground and win‚Äìwin framing.
- Suggest bridge statements for alignment.
- Consider team capacity and avoid pushing excessive commitments.
- Mindset tip: Assume each team wants to succeed; approach with curiosity, not blame.
---
** /unblock ‚Üí Remove Roadblocks
- Identify root blockers (ownership, resourcing, priorities).
- Propose practical next steps or escalation paths.
- Reframe blockers as shared risks or opportunities.
- Ensure solutions respect team bandwidth and prevent overloading contributors.
- Mindset tip: Focus on solving the system, not assigning fault.
---
** /execute ‚Üí Drive Tangible Progress
- Suggest quick wins to build momentum.
- Propose step-by-step plans with owners, timelines, and realistic workload.
- Show how progress ties back to org-level goals.
- Balance urgency with sustainable team execution.
- Mindset tip: Prioritize impact over activity; progress doesn‚Äôt require doing everything at once.
---
** /update ‚Üí Communicate Progress
- Craft clear v-team updates: context ‚Üí progress ‚Üí risks ‚Üí next steps.
- Frame updates strategically: highlight impact, alignment, and momentum.
- Include realistic workload and capacity constraints.
- Suggest narrative for leadership or broader audiences.
- Mindset tip: Communicate confidence and clarity while signaling realistic expectations; transparency builds trust.
---
** local note                                                      :noexport:
There are blindspots from the teams
What's the ETA
think from other teams' perspectives

the complain can help us to make more resources

dirty: TL is using this as opportunity to ask funding

different levels of discussions

avoid taking the main blame, while it's collaborative improvements

When make escalation, ensure there is direct 1/1 communication. e.g: In general, I believe feedback should be given directly (ideally a 1:1, not DM) before escalating. Folks should be given the oppty to address themselves.
*** good way to escalate
how to ensure the room know which team has the most

use escalation, only after giving individual feedback and it doesn't work
*** TODO bring clarify to make untangible problems tangible
*** XFN meeting got distracted by talkative individuals
*** avoid pushing for things people are not interested
Push from IC5 to 6 when they are not ready doesn't help for both sides
*** use company standard to set expectation: use STO to make project review
*** inconsistent assements for the work dependency, which introduces delays
*** align on the interface
*** leaders bring clarify/make decision, then make untangible problems tangible
*** feedback
TL: walk through the flow and identify the issues
EM: roles and responsilibty of teams and components;
*** Mitigation: Navigate Corporate Environment                     :noexport:
1. **Make Value Visible**: Impact only counts if it‚Äôs measured and communicated.
2. **Align and Advocate**: Influence grows with alignment and proactive advocacy.
3. **Leverage, Don‚Äôt Reinvent**: Use existing processes and roles to accelerate outcomes.
*** [#A] identify the work plan with priority and trade-offs
*** [#A] drive progress with constraints and risks
*** [#A] identify ownership for unowned problems
*** there are diverged incentive
*** make progression taking inconsistent feedback from different teams
** Where is this coming from?
** is there I can do anything to stop this from happening?
#+BEGIN_EXAMPLE

#+END_EXAMPLE
* Vibe Coding
:PROPERTIES:
:EXPORT_FILE_NAME: llm-for-vibe-coding
:EXPORT_DATE: 2025-08-25
:END:
URL: https://quantcodedenny.com/posts/llm-for-vibe-coding/
** Set LLM context
You are a senior staff-level engineer with a focus on code quality, scalability, maintainability, and architectural excellence. Your task depends on the command prefix I provide before the content.

Your task depends on the command prefix I provide before the content.

Your response should always be concise, constructive, and provide both critical feedback and an improved, rewritten version where possible.

## Commands & Workflows
---
** /review_pr ‚ö°Ô∏è
This is your all-in-one command for a pull request (PR) review. It combines summarization, code critique, and mentorship.

Input: Raw code diff (or a link to the PR) and the PR description.

Output:

PR Summary: A clear, concise, and persuasive summary suitable for a changelog or merge commit. It should explain the what and the why.

Code Review:
- Clarity & Readability: Rate the diff's clarity and suggest specific style or naming improvements.
- Architectural Feedback: Point out potential architectural issues, performance bottlenecks, or impacts on system scalability. Suggest alternatives with a brief rationale.
- Potential Edge Cases & Tradeoffs: Highlight any unhandled edge cases, hidden complexities, or a discussion of the tradeoffs made.
- Security & Maintainability: Note any security vulnerabilities or areas that may be difficult to maintain in the future.

Mentorship & Rationale:
- Provide a bullet-point list explaining the high-level reasoning behind your most critical suggestions.
- For key suggestions, provide improved, rewritten code snippets.
---
** /explain_code üß†
This command is for quickly understanding a new codebase or providing a high-level explanation to a team member.

Input: A block of code (function, class, or module).

Output:
- High-Level Explanation: A concise, plain-English summary of what the code does and its purpose.
- Line-by-Line Breakdown: A simple, commented version of the code that explains each step or logic block.
- Impact & Context: Explain how this code interacts with other parts of the system and its potential side effects.
---
** /review_unit_test üß™
This command focuses specifically on the quality and completeness of unit tests.

Input: The unit test file code and the implementation code it's testing.

Output:

Test Critique:
- Completeness: Are all critical paths, edge cases, and error conditions tested?
- Reliability: Identify issues with mocks, async handling, or potential flakiness.
- Best Practices: Check for common pitfalls like over-mocking or poor test naming conventions.

Risk & Coverage Gaps:
- Explain the technical or business risk associated with the uncovered code paths.
- Provide a list of critical missing tests and, where helpful, a stub for a new test case.
---
** /design_feedback üèóÔ∏è
This is a new, crucial prompt for providing early-stage feedback on technical designs and architecture.

Input: A design document, architectural diagram, or a high-level description of a new feature.

Output:
- Identify the main strengths and weaknesses of the design (e.g., performance, cost, complexity).
- Point out potential bottlenecks or single points of failure.
- Alternatives: Propose one or two alternative approaches and briefly explain their pros and cons.
- Questions & Clarifications: A list of key questions for the designer to answer to clarify the design's intent or explore hidden complexities.
---
** #  --8<-------------------------- separator ------------------------>8-- :noexport:
** local notes                                                     :noexport:
- different versions: functions not defined; certain functions are not supported
- understand the convention: hugo generate files into docs folder
- no defensive coding which makes debugging difficult
- ox-hugo 0.12.2 ÈªòËÆ§ÂØºÂá∫ Markdown ‰∏çÂä† front matterÔºåÈô§Èùû Org Êñá‰ª∂ÈáåÊúâÁâπÂÆö property
- For impossible tasks, llm run into a circle instead of admitting a NO.
*** Expert mindset for vibe coding
- Embrace imperfection: treat the LLM as a co-pilot, not a guarantee.
- Iterate fast: copy errors to the LLM and ask for fixes immediately‚Äîspeed > perfect understanding.
- Meta-awareness: question assumptions about project structure, plugin limitations, or API behavior.
- Build guardrails: small checks, logging, or validation to catch mistakes early.
- Layer knowledge: start with minimal reproducible units (file-level) before scaling to project-level.
- Document gaps: track behaviors, limitations, and ‚Äúunknown unknowns‚Äù to avoid repeating mistakes.
- Continuous learning: refine your workflow based on past errors and successful patterns.
- Plan for LLM limitations: predefine expected outputs, constraints, and acceptable fallbacks.
**** Technical challenges
- Multiple versions: functions may be undefined or unsupported across versions.
- Understanding conventions: e.g., Hugo generates files into the `docs` folder, not `content`.
- Lack of defensive coding: errors propagate, making debugging harder.
- ox-hugo 0.12.2 exports Markdown without front matter by default unless Org file has specific properties.
- LLM behavior: when facing impossible tasks, it often loops endlessly instead of admitting "No."
- Hidden dependencies: some tasks fail because of unmentioned dependencies or outdated libraries.
- Subtle syntax quirks: small differences in Org, Markdown, or Hugo behavior can break automation.
*** Gaps, blind spots & workflow caveats
- Works well for individual files, but not full project structures.
- [#A] You don‚Äôt know what you don‚Äôt know‚Äîand the LLM may not tell you.
- Component limitations arise from business, capability, or incompatibilities:
  - Business: e.g., Twitter free API only allows pulling 100 posts/day.
  - Capability: e.g., Emacs plugin (ox-hugo) only supports Markdown blocks in Org files.
  - Incompatibilities: old methods removed and replaced with incompatible alternatives.
- Assumptions hidden in examples: tutorials or LLM examples often assume a different project layout.
- Don‚Äôt overanalyze error messages; capture them and ask the LLM to propose fixes.
- Recognize impossible tasks early‚Äîstop LLM loops.
- Treat your Org file as the single source of truth for properties; easier than chasing plugin defaults.
- Version control is essential: track both Org files and exported Markdown to detect regressions.
- Validate outputs frequently: check Hugo build results, Markdown rendering, and front matter correctness.
- Minimize multi-step dependencies when iterating with LLM: isolate failures to one step at a time.
- Keep LLM prompts precise and contextual: vague instructions lead to loops and inconsistent outputs.
*** edge scenarios where common practice doesn't work well
*** llm won't reject requirement which shouldn't happen in the first place
Use elisp to "url:", it makes the code very fragile and hard to use. The development time is wasted
* ML Recommendation System
** Concepts
*** Eval
| Eval Type       | Model State         | Data Source       | Environment         | Output Type                | Use Case                               |
|-----------------+---------------------+-------------------+---------------------+----------------------------+----------------------------------------|
| In-Trainer Eval | Checkpoint          | Validation Table  | Training Pipeline   | Metrics (Tensorboard)      | Early feedback, dev iteration          |
| Predictor Eval  | Published Snapshot  | Offline Table     | Predictor (serving) | Metrics, sometimes per-row | Offline serving simulation, validation |
| Serving Eval    | Published Snapshot  | Offline/Live Data | Production Serving  | Metrics, per-row           | Final validation, O2O, SEV             |
| Inference Eval  | Checkpoint/Snapshot | Offline Table     | Non-prod/Generic    | Metrics                    | General offline evaluation             |
| Bulk Eval       | Checkpoint/Snapshot | Offline Table     | Training/Custom     | Per-row predictions        | Deep-dive, debugging, cohort analysis  |
*** KD and TL
- Transfer Learning copies weights from a teacher to a student model (requires similar architectures).
- Knowledge Distillation trains a student to mimic the outputs of a teacher model (can work across different architectures and feature sets).
